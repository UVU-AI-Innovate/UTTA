# üéì Educational Fine-tuning Methods for LLMs

This guide explains how different fine-tuning approaches can help create an educational chatbot that simulates a second-grade student. Each method has unique characteristics that affect how your chatbot learns and behaves.

## üéØ Learning Objectives

After working with these examples, you will understand:
1. How different fine-tuning methods shape your chatbot's behavior
2. Which datasets work best for each approach
3. How to choose the right method for your educational chatbot

## ü§ñ Fine-tuning Methods and Their Effects

### 1. DSPy: Teaching Through Better Prompting
**What It Does:**
- Helps your chatbot think like a second-grade student without changing its core knowledge
- Uses smart prompting to guide responses toward age-appropriate language and reasoning
- Maintains consistency in the student persona

**Dataset Requirements:**
- Small dataset (10-50 examples)
- Focus on quality over quantity
- Examples should show:
  ```json
  {
    "dialogue": [
      {"role": "teacher", "content": "What happens to water when it gets very cold?"},
      {"role": "student", "content": "It turns into ice! I saw it in my freezer."},
      {"role": "teacher", "content": "That's right! What else do you know about ice?"}
    ]
  }
  ```
- Include various subjects (math, science, reading)
- Show typical second-grade reasoning patterns

**Effect on Chatbot:**
- More natural, child-like responses
- Better at showing age-appropriate understanding
- Can express common misconceptions naturally
- Good at maintaining consistent personality

### 2. OpenAI Fine-tuning: Teaching New Knowledge
**What It Does:**
- Actually teaches your chatbot new information and response patterns
- Helps it understand grade-specific content
- Builds consistent response patterns

**Dataset Requirements:**
- Medium dataset (50-100+ examples)
- Structured conversations showing:
  ```json
  {
    "messages": [
      {"role": "system", "content": "You are a second-grade student who..."},
      {"role": "user", "content": "Can you solve this math problem: 15 + 8 = ?"},
      {"role": "assistant", "content": "Let me count... First I get to 15, then I count 8 more: 16, 17, 18... It's 23!"}
    ]
  }
  ```
- Include:
  - Grade-appropriate vocabulary
  - Step-by-step thinking
  - Common second-grade mistakes
  - Various subject areas

**Effect on Chatbot:**
- More consistent grade-level responses
- Better at specific subject areas
- Can show learning progress
- Maintains educational context well

### 3. LoRA Fine-tuning: Deep Personality Training
**What It Does:**
- Deeply modifies how your chatbot thinks and responds
- Creates a more permanent second-grade student personality
- Allows for complex behavior patterns

**Dataset Requirements:**
- Larger dataset (100+ examples)
- Detailed interactions showing:
  ```json
  {
    "instruction": "Respond to this question as a second-grade student: What makes the sky blue?",
    "output": "My teacher said something about light and colors... I think the blue light bounces around more in the air. It's like when I use my blue crayon to color the sky in my drawings!"
  }
  ```
- Should include:
  - Various thinking patterns
  - Different emotional states
  - Multiple subjects
  - Social interactions

**Effect on Chatbot:**
- Most consistent personality
- Better at complex interactions
- Can handle unexpected questions well
- More natural learning progression

## üìö Choosing the Right Method

Consider these factors when selecting a method:

1. **Learning Goals**
   - DSPy: Best for quick personality adaptation
   - OpenAI: Good for specific subject knowledge
   - LoRA: Best for deep personality development

2. **Available Data**
   - Limited examples ‚Üí DSPy
   - Moderate collection ‚Üí OpenAI
   - Large dataset ‚Üí LoRA

3. **Development Time**
   - Quick results ‚Üí DSPy
   - Moderate setup ‚Üí OpenAI
   - Longer development ‚Üí LoRA

## üéì Educational Considerations

When implementing your chatbot, focus on:

1. **Age-Appropriate Responses**
   - Use grade-level vocabulary
   - Show appropriate reasoning skills
   - Include common misconceptions
   - Maintain consistent knowledge level

2. **Learning Progression**
   - Show natural learning curves
   - Include "aha moments"
   - Demonstrate growing understanding
   - Allow for mistakes and corrections

3. **Subject Matter**
   - Cover grade-level topics
   - Show connections between subjects
   - Include real-world examples
   - Demonstrate curiosity and questions

## üìä Expert Evaluation Process

Your team's implementation will be evaluated through a presentation to your assigned elementary teacher professor. These experts specialize in elementary education and will assess how well your chatbot simulates a second-grade student.


### What Professors Will Evaluate

1. **Student Simulation Authenticity**
   - Age-appropriate vocabulary and language
   - Typical second-grade reasoning patterns
   - Common misconceptions and learning challenges
   - Natural conversation flow

2. **Educational Value**
   - Alignment with second-grade curriculum
   - Learning progression demonstration
   - Subject matter understanding
   - Engagement level

## üéØ Project-Based Learning Approach

This course follows a project-based learning approach where you'll:

1. **Learn from Examples**
   - Study and run provided example implementations
   - Understand key concepts through practical demonstrations
   - Analyze real-world educational dialogue scenarios

2. **Apply to Your Project**
   - Choose a domain for your own fine-tuning project
   - Create custom datasets for your use case
   - Implement fine-tuning methods in your application
   - Evaluate and optimize performance

3. **Build Production Systems**
   - Deploy fine-tuned models
   - Monitor performance
   - Manage costs
   - Scale your solution

## üõ†Ô∏è Environment Setup

### Prerequisites
- Python 3.10 or higher
- Conda package manager
- OpenAI API key (for DSPy and OpenAI approaches)
- Git (for cloning the repository)

### Setting up the Environment

1. **Clone the Repository**
   ```bash
   git clone <repository-url>
   cd <repository-name>
   ```

2. **Create and Activate the Conda Environment**
   ```bash
   # Create the environment from environment.yml
   conda env create -f environment.yml

   # Activate the environment
   conda activate utta
   ```

   The `environment.yml` file includes all necessary dependencies:
   - dspy-ai: For prompt optimization
   - openai: For cloud-based fine-tuning
   - torch: For local model training
   - transformers: For HuggingFace models
   - peft: For LoRA fine-tuning
   - Additional utilities and dependencies

3. **Verify Installation**
   ```bash
   # Check if key packages are installed
   python -c "import dspy; import openai; import torch; print('Setup successful!')"
   ```

### OpenAI API Key Configuration

The OpenAI API key is required for:
- DSPy approach: For making API calls during prompt optimization
- OpenAI fine-tuning: For both training and using the fine-tuned model

Set up your API key in one of these ways:

1. **Environment Variable (Recommended for Development)**
   ```bash
   # Linux/macOS
   export OPENAI_API_KEY='your-api-key'
   
   # Windows (PowerShell)
   $env:OPENAI_API_KEY='your-api-key'
   ```

2. **Create a .env File (Recommended for Project)**
   ```bash
   # Create .env in the project root
   echo "OPENAI_API_KEY=your-api-key" > .env
   ```

3. **Direct Configuration in Python (Not Recommended)**
   ```python
   import openai
   openai.api_key = 'your-api-key'  # Not recommended for production
   ```

### Cost Management

1. **DSPy Approach**
   - API calls during optimization: ~$0.01-$0.20 per cycle
   - Can use cheaper models (e.g., GPT-3.5-turbo)
   - Best for: Quick experiments and immediate results
   - Cost reduction: Use caching and cheaper models

2. **OpenAI Fine-tuning**
   - Training: $0.008 per 1K training tokens
   - Inference: 1.5-2x base model cost
   - Best for: Production-ready educational chatbots
   - Cost reduction: Optimize dataset size, use GPT-3.5

3. **HuggingFace LoRA**
   - No API costs (runs locally)
   - Requires computational resources
   - Best for: Long-term deployment and data privacy
   - Cost reduction: Use smaller models, quantization

## üìä Creating Effective Fine-tuning Datasets

The success of your chatbot largely depends on the quality and structure of your training datasets. Each fine-tuning method requires specific dataset characteristics to effectively simulate a second-grade student.

### 1. DSPy Dataset Creation (10-50 Examples)
**Dataset Structure:**
```json
{
  "dialogue": [
    {"role": "teacher", "content": "What happens when you add 8 + 7?"},
    {"role": "student", "content": "Let me count... 8, then 9, 10, 11, 12, 13, 14, 15! It's 15!"},
    {"role": "teacher", "content": "Good counting! Can you think of another way to solve it?"},
    {"role": "student", "content": "Oh! I know 8 + 2 is 10, and then 5 more makes 15!"}
  ]
}
```

**Key Elements to Include:**
1. **Thinking Process**
   - Show step-by-step reasoning
   - Include self-corrections
   - Demonstrate learning moments

2. **Language Level**
   - Use simple sentences
   - Include age-appropriate vocabulary
   - Show occasional grammar mistakes

3. **Subject Coverage**
   - Basic math operations
   - Simple science concepts
   - Reading comprehension
   - Social studies topics

**Collection Tips:**
- Record actual second-grade classroom interactions
- Observe how students explain concepts to peers
- Include common misconceptions and corrections
- Show different learning styles

### 2. OpenAI Fine-tuning Dataset (50-100+ Examples)
**Dataset Structure:**
```json
{
  "messages": [
    {"role": "system", "content": "You are a second-grade student who is curious, sometimes makes mistakes, and learns through exploration. You use simple words and occasionally need help with complex concepts."},
    {"role": "user", "content": "Can you explain how plants grow?"},
    {"role": "assistant", "content": "Plants need water and sunlight to grow! My class did an experiment where we put seeds in cups. Some got water and sun, some didn't. The ones with water and sun grew into little plants! But the others didn't grow at all. My teacher said they also need soil with food in it, but I'm not sure what kind of food plants like."}
  ]
}
```

**Essential Components:**
1. **System Messages**
   - Define student personality
   - Set knowledge boundaries
   - Establish speech patterns
   - Include common behaviors

2. **Conversation Flows**
   - Question-answer patterns
   - Follow-up questions
   - Misconception corrections
   - Knowledge building

3. **Subject Areas**
   - Grade-level math problems
   - Basic scientific concepts
   - Reading and writing skills
   - Social interactions

**Data Collection Strategy:**
1. Start with Core Subjects
   - Document real classroom Q&A
   - Include homework discussions
   - Record peer explanations
   - Capture teacher-student interactions

2. Add Variety
   - Different times of day
   - Various emotional states
   - Multiple learning scenarios
   - Different difficulty levels

### 3. LoRA Dataset Creation (100+ Examples)
**Dataset Structure:**
```json
{
  "instruction": "You are helping your friend understand why seasons change. How would you explain it?",
  "output": "Well, my teacher showed us with a globe and a flashlight! When our part of Earth faces the sun more, it's summer and we get more warm days. Then when it faces away more, it's winter and gets cold. It's like when I stand close to a heater - I feel warmer! But when I move away, I feel colder. I'm still learning about it, but I think that's why we have different seasons!"
}
```

**Critical Components:**
1. **Comprehensive Coverage**
   - Multiple subject areas
   - Various difficulty levels
   - Different interaction types
   - Range of emotional responses

2. **Response Variations**
   - Confident answers
   - Uncertain responses
   - Partial knowledge
   - Learning progression

3. **Personality Consistency**
   - Age-appropriate reactions
   - Typical misconceptions
   - Learning patterns
   - Social interactions

**Collection Methods:**
1. Systematic Documentation
   - Record daily class interactions
   - Note common questions
   - Document learning struggles
   - Track progress patterns

2. Quality Control
   - Verify grade-level appropriateness
   - Check vocabulary usage
   - Ensure consistent personality
   - Validate with teachers

### Dataset Creation Best Practices

1. **Quality Standards**
   - Authentic student language
   - Age-appropriate reasoning
   - Consistent personality
   - Natural conversation flow

2. **Validation Process**
   - Review by grade teachers
   - Test with sample prompts
   - Check for consistency
   - Verify educational value

3. **Organization Tips**
   - Label by subject area
   - Mark difficulty levels
   - Note special cases
   - Track source/context

4. **Common Pitfalls to Avoid**
   - Too advanced vocabulary
   - Inconsistent personality
   - Unrealistic knowledge
   - Adult-like reasoning

### Dataset Testing and Iteration

1. **Initial Testing**
   - Start with small samples
   - Test basic interactions
   - Check personality consistency
   - Verify grade-level appropriateness

2. **Refinement Process**
   - Gather teacher feedback
   - Adjust language level
   - Add missing scenarios
   - Remove inconsistencies

3. **Validation Metrics**
   - Vocabulary grade level
   - Response consistency
   - Knowledge accuracy
   - Personality alignment

## üí° Project Implementation Guide

### 1. DSPy Implementation (Week 1-2)
1. **Setup and Exploration**
   ```python
   # Example: Custom DSPy signature for your domain
   class YourDomainSignature(dspy.Signature):
       context = dspy.InputField()
       query = dspy.InputField()
       response = dspy.OutputField()
   ```
   - Study dspy_example.py
   - Adapt signatures for your domain
   - Test basic optimizations

2. **Custom Development**
   - Create domain-specific metrics
   - Implement custom chains
   - Optimize for your use case

### 2. OpenAI Fine-tuning (Week 3-4)
1. **Initial Setup**
   ```python
   # Example: Prepare your training data
   def prepare_domain_data(your_data):
       return [{
           "messages": [
               {"role": "system", "content": YOUR_SYSTEM_PROMPT},
               {"role": "user", "content": item["input"]},
               {"role": "assistant", "content": item["output"]}
           ]
       } for item in your_data]
   ```
   - Study openai_finetune.py
   - Adapt for your domain
   - Test with small datasets

2. **Production Development**
   - Implement data pipeline
   - Set up monitoring
   - Optimize costs
   - Scale gradually

### 3. LoRA Implementation (Week 5-6)
1. **Local Setup**
   ```python
   # Example: Custom LoRA configuration
   config = LoraConfig(
       r=8,
       lora_alpha=32,
       target_modules=["q_proj", "v_proj"],
       lora_dropout=0.05,
       bias="none",
       task_type="CAUSAL_LM"
   )
   ```
   - Study huggingface_lora.py
   - Configure for your resources
   - Test training pipeline

2. **Optimization**
   - Tune hyperparameters
   - Optimize memory usage
   - Implement quantization
   - Deploy efficiently


## üìÇ Repository Structure

```
examples/
‚îú‚îÄ‚îÄ README.md                                 # This file - Overview and instructions
‚îÇ
‚îú‚îÄ‚îÄ Approach 1: DSPy Prompt Optimization
‚îÇ   ‚îú‚îÄ‚îÄ dspy_example.py                      # Main DSPy implementation
‚îÇ   ‚îú‚îÄ‚îÄ teacher_student_dialogues.jsonl      # Training dialogues
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                             # Helper functions
‚îÇ
‚îú‚îÄ‚îÄ Approach 2: OpenAI Fine-tuning
‚îÇ   ‚îú‚îÄ‚îÄ openai_finetune.py                   # OpenAI fine-tuning implementation
‚îÇ   ‚îú‚îÄ‚îÄ openai_teacher_dialogue_training.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ openai_teacher_dialogue_validation.jsonl
‚îÇ
‚îî‚îÄ‚îÄ Approach 3: HuggingFace LoRA
    ‚îú‚îÄ‚îÄ huggingface_lora.py                  # LoRA implementation
    ‚îú‚îÄ‚îÄ small_edu_qa.jsonl                   # Sample QA dataset
    ‚îî‚îÄ‚îÄ openai_edu_qa_training.jsonl         # Additional training data
```

## üîç Three Approaches Explained

### 1. DSPy: Prompt Optimization (dspy_example.py)

**Key Features:**
- No model weight changes - only better prompting
- Works with smaller datasets (10-50 dialogue examples)
- Uses Chain-of-Thought prompting for pedagogical reasoning
- API-based approach with immediate results
- Includes evaluation metrics for teaching quality

**Implementation Details:**
```python
class StudentResponse(dspy.Signature):
    """Define the input/output structure for student responses"""
    conversation_history = dspy.InputField()
    teacher_prompt = dspy.InputField()
    student_response = dspy.OutputField()

class StudentSimulationModel(dspy.Module):
    """Implement the student simulation logic"""
    def __init__(self):
        # Initialize with chain-of-thought reasoning
        self.chain = dspy.ChainOfThought(StudentResponse)
```

### 2. OpenAI: Cloud Fine-tuning (openai_finetune.py)

**Key Features:**
- Updates actual model weights through cloud service
- Requires medium-sized datasets (50-100+ dialogue examples)
- Balance of control and convenience
- Handles multi-turn conversations with context preservation

**Implementation Details:**
```python
def prepare_training_data(dialogues):
    """Convert educational dialogues to OpenAI format"""
    return [{
        "messages": [
            {"role": "system", "content": TEACHER_SYSTEM_PROMPT},
            {"role": "user", "content": dialogue["student"]},
            {"role": "assistant", "content": dialogue["teacher"]}
        ]
    } for dialogue in dialogues]
```

### 3. HuggingFace: Local LoRA (huggingface_lora.py)

**Key Features:**
- Full local control over model and training process
- Uses LoRA for efficient adaptation of model weights
- Works best with larger datasets (100-1000+ dialogue examples)
- Data privacy - all information stays on your machine

**Implementation Details:**
```python
def prepare_model_and_tokenizer():
    """Set up base model with LoRA configuration"""
    model = AutoModelForCausalLM.from_pretrained(
        "base_model",
        load_in_8bit=True,
        device_map="auto"
    )
    
    config = LoraConfig(
        r=8,  # Rank of update matrices
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
```

## üìö Resources

- DSPy Documentation: [https://dspy.ai/](https://dspy.ai/)
- OpenAI Fine-tuning Guide: [https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)
- HuggingFace LoRA Guide: [https://huggingface.co/docs/peft/conceptual_guides/lora](https://huggingface.co/docs/peft/conceptual_guides/lora)

## ü§ù Support

If you encounter issues:
1. Check the documentation in each example file
2. Review the code comments
3. Contact course staff through designated channels

Good luck with your learning journey! üöÄ
