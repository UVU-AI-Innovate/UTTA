<div class="markdown-heading">
    <div class="markdown-heading">
        <h2 class="heading-element">Introduction</h2>
        <a id="user-content-introduction" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#introduction" aria-label="Permalink: Introduction"></a>
    </div>
    <p>In this assignment, you will explore three different approaches to improving LLM performance on a specific task: educational question answering. You'll work with the provided examples to gain hands-on experience with prompt optimization, cloud-based fine-tuning, and local fine-tuning with parameter-efficient techniques.</p>
    <p>All files needed for this assignment are available at:<span>&nbsp;</span><a href="https://github.com/UVU-AI-Innovate/UTTA/tree/main/examples">https://github.com/UVU-AI-Innovate/UTTA/tree/main/examples</a></p>
    <div class="markdown-heading">
        <h2 class="heading-element">Learning Objectives</h2>
        <a id="user-content-learning-objectives" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#learning-objectives" aria-label="Permalink: Learning Objectives"></a>
    </div>
    <ul>
        <li>Understand the key differences between prompt optimization, cloud fine-tuning, and local fine-tuning</li>
        <li>Gain practical experience with DSPy, OpenAI fine-tuning, and HuggingFace LoRA</li>
        <li>Analyze tradeoffs in data requirements, computational resources, and model performance</li>
        <li>Develop skills in evaluating which approach is most suitable for different scenarios</li>
    </ul>
    <div class="markdown-heading">
        <h2 class="heading-element">Prerequisites</h2>
        <a id="user-content-prerequisites" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#prerequisites" aria-label="Permalink: Prerequisites"></a>
    </div>
    <ul>
        <li>Python 3.10 (required by the conda environment)</li>
        <li>Access to the example files:<span>&nbsp;</span><code>dspy_example.py</code>,<span>&nbsp;</span><code>openai_finetune.py</code>,<span>&nbsp;</span><code>huggingface_lora.py</code>, and<span>&nbsp;</span><code>simple_example.py</code></li>
        <li>OpenAI API key (for Parts 1 and 2)</li>
        <li>Conda environment manager</li>
    </ul>
    <p><strong>Important Note</strong>: No GPU is required for any part of this assignment. All examples and tasks can be completed on any standard computer using the provided conda environment.</p>
    <div class="markdown-heading">
        <h3 class="heading-element">Environment Setup</h3>
        <a id="user-content-environment-setup" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#environment-setup" aria-label="Permalink: Environment Setup"></a>
    </div>
    <p>We've provided an <code>environment.yml</code> file with all necessary dependencies. To set up your environment:</p>
    <ol>
        <li>
            <p><strong>Install Conda</strong> if you don't have it already: <a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda</a> or <a href="https://www.anaconda.com/products/distribution">Anaconda</a></p>
        </li>
        <li>
            <p><strong>Create and activate the conda environment</strong>:</p>
            <pre><code>
# From the project root directory
conda env create -f environment.yml
conda activate utta
            </code></pre>
        </li>
        <li>
            <p><strong>Verify installation</strong>:</p>
            <pre><code>
# Check that key packages are available
python -c "import dspy; import torch; import openai; print('Environment successfully configured!')"
            </code></pre>
        </li>
    </ol>
    <p>The environment includes:</p>
    <ul>
        <li>Python 3.10</li>
        <li>PyTorch 2.2.0</li>
        <li>DSPy 2.0.4</li>
        <li>OpenAI 0.28.0</li>
        <li>Other necessary libraries for the examples</li>
    </ul>
    <div class="markdown-heading">
        <h3 class="heading-element">OpenAI API Setup</h3>
        <a id="user-content-openai-api-setup" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#openai-api-setup" aria-label="Permalink: OpenAI API Setup"></a>
    </div>
    <p>For Parts 1 and 2, you'll need to set up your OpenAI API key:</p>
    <ol>
        <li>
            <p><strong>Get an API key</strong> from <a href="https://platform.openai.com/api-keys">OpenAI's platform</a> if you don't already have one</p>
        </li>
        <li>
            <p><strong>Create a .env file</strong> in the project root directory:</p>
            <pre><code>
# .env file contents
OPENAI_API_KEY=your_api_key_here
            </code></pre>
        </li>
        <li>
            <p><strong>Test your API key</strong>:</p>
            <pre><code>
python -c "import os; from dotenv import load_dotenv; import openai; load_dotenv(); openai.api_key = os.getenv('OPENAI_API_KEY'); print('API key loaded successfully!')"
            </code></pre>
        </li>
        <li>
            <p><strong>Cost Management</strong>: To minimize costs, use the <code>gpt-3.5-turbo</code> model for all API calls. This model is specified in the examples and provides a good balance of capabilities and cost-effectiveness.</p>
        </li>
    </ol>
    <div class="markdown-heading">
        <h2 class="heading-element">Assignment Overview</h2>
        <a id="user-content-assignment-overview" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#assignment-overview" aria-label="Permalink: Assignment Overview"></a>
    </div>
    <p>This assignment is structured in three parts of increasing complexity:</p>
    <ol>
        <li><strong>Part 1: DSPy Prompt Optimization</strong><span>&nbsp;</span>- Modify and extend the DSPy example</li>
        <li><strong>Part 2: OpenAI Fine-Tuning</strong><span>&nbsp;</span>- Prepare data and set up an OpenAI fine-tuning job</li>
        <li><strong>Part 3: HuggingFace LoRA Fine-Tuning</strong><span>&nbsp;</span>- Run a parameter-efficient fine-tuning task locally <em>(can be completed without a GPU)</em></li>
    </ol>
    <p>Before starting, we recommend running the <code>simple_example.py</code> script which provides a concise overview of all three approaches and their key differences.</p>
    <div class="markdown-heading">
        <h2 class="heading-element">Detailed Tasks</h2>
        <a id="user-content-detailed-tasks" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#detailed-tasks" aria-label="Permalink: Detailed Tasks"></a>
    </div>
    <div class="markdown-heading">
        <h3 class="heading-element">Part 1: DSPy Prompt Optimization (3-4 hours)</h3>
        <a id="user-content-part-1-dspy-prompt-optimization-3-4-hours" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#part-1-dspy-prompt-optimization-3-4-hours" aria-label="Permalink: Part 1: DSPy Prompt Optimization (3-4 hours)"></a>
    </div>
    <ol>
        <li>
            <p><strong>Setup and Exploration</strong></p>
            <ul>
                <li>Run the DSPy example and analyze the outputs</li>
                <li>Identify where in the code the chain-of-thought reasoning is implemented</li>
                <li>Examine the <code>teacher_student_dialogues.jsonl</code> file format</li>
            </ul>
        </li>
        <li>
            <p><strong>Dataset Extension</strong></p>
            <ul>
                <li>Create 5 additional educational QA pairs on a topic of your choice</li>
                <li>Add them to the dataset and test their performance</li>
            </ul>
        </li>
        <li>
            <p><strong>DSPy Module Enhancement</strong></p>
            <ul>
                <li>Modify the<span>&nbsp;</span><code>EducationalQAModel</code><span>&nbsp;</span>to incorporate one of the following:
                    <ul>
                        <li>Retrieval-augmented generation</li>
                        <li>Few-shot learning with examples</li>
                        <li>Structured reasoning steps</li>
                    </ul>
                </li>
            </ul>
        </li>
        <li>
            <p><strong>Evaluation</strong></p>
            <ul>
                <li>Develop a more sophisticated evaluation metric than the current simple word matching</li>
                <li>Test both the original and your enhanced model on the same test set</li>
                <li>Record performance differences</li>
            </ul>
        </li>
    </ol>
    <p><strong>Deliverable</strong>: Modified<span>&nbsp;</span><code>dspy_enhanced.py</code><span>&nbsp;</span>script with your improvements and a short report on performance changes.</p>
    <div class="markdown-heading">
        <h3 class="heading-element">Part 2: OpenAI Fine-Tuning (4-5 hours)</h3>
        <a id="user-content-part-2-openai-fine-tuning-4-5-hours" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#part-2-openai-fine-tuning-4-5-hours" aria-label="Permalink: Part 2: OpenAI Fine-Tuning (4-5 hours)"></a>
    </div>
    <ol>
        <li>
            <p><strong>Dataset Preparation</strong></p>
            <ul>
                <li>Examine the <code>openai_edu_qa_training.jsonl</code> sample file to understand the format</li>
                <li>Create a specialized dataset of 15-20 QA pairs focused on a single domain (e.g., biology, history)</li>
                <li>Format this dataset appropriately for OpenAI fine-tuning</li>
            </ul>
        </li>
        <li>
            <p><strong>Fine-Tuning Setup</strong></p>
            <ul>
                <li>Uncomment and modify the fine-tuning section of the provided example</li>
                <li>Add proper error handling and status monitoring</li>
                <li>(Optional with instructor approval) Run a small fine-tuning job with OpenAI</li>
            </ul>
        </li>
        <li>
            <p><strong>Simulation and Testing</strong></p>
            <ul>
                <li>If not actually running the fine-tuning job, create a simulation component that mimics expected improvements</li>
                <li>Design a test set of questions that evaluates domain knowledge in your chosen field</li>
            </ul>
        </li>
        <li>
            <p><strong>Cost-Benefit Analysis</strong></p>
            <ul>
                <li>Calculate the approximate cost of fine-tuning with your dataset</li>
                <li>Estimate the cost per query for both fine-tuned and non-fine-tuned models</li>
                <li>Determine at what usage volume fine-tuning becomes cost-effective</li>
            </ul>
        </li>
    </ol>
    <p><strong>Deliverable</strong>: Your<span>&nbsp;</span><code>openai_domain_finetune.py</code><span>&nbsp;</span>script, dataset file, and a cost-benefit analysis document.</p>
    <div class="markdown-heading">
        <h3 class="heading-element">Part 3: HuggingFace LoRA Fine-Tuning (6-8 hours)</h3>
        <a id="user-content-part-3-huggingface-lora-fine-tuning-6-8-hours" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#part-3-huggingface-lora-fine-tuning-6-8-hours" aria-label="Permalink: Part 3: HuggingFace LoRA Fine-Tuning (6-8 hours)"></a>
    </div>
    <ol>
        <li>
            <p><strong>Environment Setup</strong></p>
            <ul>
                <li>Properly configure a Python environment for transformer-based models using the provided conda environment</li>
                <li>Note that you can complete this part without a GPU by adjusting hyperparameters and model size</li>
                <li>Study the <code>small_edu_qa.jsonl</code> file to understand the data format</li>
            </ul>
        </li>
        <li>
            <p><strong>Model Selection and Optimization</strong></p>
            <ul>
                <li>Choose an appropriate base model from HuggingFace (smaller than the example if needed)</li>
                <li>For CPU-only training, consider using smaller models like DistilBERT or TinyLlama</li>
                <li>Experiment with different LoRA hyperparameters (rank, alpha, target modules)</li>
                <li>Use lower precision (8-bit quantization) and smaller batch sizes to reduce memory requirements</li>
                <li>Implement training with proper checkpointing and early stopping</li>
            </ul>
        </li>
        <li>
            <p><strong>Training and Evaluation</strong></p>
            <ul>
                <li>Train your LoRA adapter on the educational QA dataset</li>
                <li>For CPU training, limit the dataset size and number of training steps</li>
                <li>Evaluate performance before and after adaptation</li>
                <li>Measure inference speed and memory usage</li>
            </ul>
        </li>
        <li>
            <p><strong>Adapter Management</strong></p>
            <ul>
                <li>Implement proper saving and loading of your LoRA adapter</li>
                <li>Create a simple inference script that loads only the necessary components</li>
            </ul>
        </li>
    </ol>
    <p><strong>Note</strong>: If you're working without a GPU, focus on understanding the PEFT/LoRA concepts and code implementation rather than achieving state-of-the-art performance. You can still experiment with and demonstrate the core concepts using smaller models and datasets.</p>
    <p><strong>Deliverable</strong>: Your<span>&nbsp;</span><code>huggingface_custom_lora.py</code><span>&nbsp;</span>script, trained adapter files, and performance metrics.</p>
    <div class="markdown-heading">
        <h2 class="heading-element">Comparative Analysis (Required for all students)</h2>
        <a id="user-content-comparative-analysis-required-for-all-students" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#comparative-analysis-required-for-all-students" aria-label="Permalink: Comparative Analysis (Required for all students)"></a>
    </div>
    <p>After completing at least two parts of the assignment, write a 2-3 page analysis comparing the approaches. Address:</p>
    <ol>
        <li>Development time and effort required</li>
        <li>Data preparation differences</li>
        <li>Performance on identical test questions</li>
        <li>Resource requirements (compute, API costs)</li>
        <li>Flexibility for adaptation to new domains</li>
        <li>Practical considerations for deployment</li>
    </ol>
    <div class="markdown-heading">
        <h2 class="heading-element">Bonus Challenges</h2>
        <a id="user-content-bonus-challenges" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#bonus-challenges" aria-label="Permalink: Bonus Challenges"></a>
    </div>
    <ol>
        <li><strong>Hybrid Approach</strong>: Combine DSPy prompt optimization with one of the fine-tuning methods</li>
        <li><strong>Extended Evaluation</strong>: Create a benchmark of 50+ questions and evaluate all three approaches</li>
        <li><strong>Model Distillation</strong>: Use a fine-tuned model to generate training data for a smaller model</li>
        <li><strong>Run All Examples</strong>: Modify the <code>run_all_examples.sh</code> script to run your enhanced versions of all three approaches</li>
    </ol>
    <div class="markdown-heading">
        <h2 class="heading-element">Submission Requirements</h2>
        <a id="user-content-submission-requirements" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#submission-requirements" aria-label="Permalink: Submission Requirements"></a>
    </div>
    <ol>
        <li>Code files for each part you completed</li>
        <li>Dataset files created or modified</li>
        <li>Comparative analysis document</li>
        <li>Brief reflection on what you learned (1 page)</li>
        <li>Any trained model artifacts or adapter files</li>
    </ol>
    <div class="markdown-heading">
        <h2 class="heading-element">Evaluation Criteria</h2>
        <a id="user-content-evaluation-criteria" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#evaluation-criteria" aria-label="Permalink: Evaluation Criteria"></a>
    </div>
    <ul>
        <li><strong>Functionality</strong><span>&nbsp;</span>(40%): Does your code work as expected?</li>
        <li><strong>Understanding</strong><span>&nbsp;</span>(30%): Does your analysis demonstrate comprehension of the key differences?</li>
        <li><strong>Innovation</strong><span>&nbsp;</span>(15%): Have you added meaningful improvements to the base examples?</li>
        <li><strong>Documentation</strong><span>&nbsp;</span>(15%): Is your code well-commented and your analysis clear?</li>
    </ul>
    <div class="markdown-heading">
        <h2 class="heading-element">Resources</h2>
        <a id="user-content-resources" class="anchor" href="https://github.com/UVU-AI-Innovate/UTTA/blob/main/examples/assingment.md#resources" aria-label="Permalink: Resources"></a>
    </div>
    <ul>
        <li><strong>Example Files</strong>:</li>
        <li style="list-style-type: none; margin-left: 20px;">- <code>simple_example.py</code> - Quick overview of all three approaches</li>
        <li style="list-style-type: none; margin-left: 20px;">- <code>dspy_example.py</code> - DSPy prompt optimization implementation</li>
        <li style="list-style-type: none; margin-left: 20px;">- <code>openai_finetune.py</code> - OpenAI fine-tuning implementation</li>
        <li style="list-style-type: none; margin-left: 20px;">- <code>huggingface_lora.py</code> - HuggingFace LoRA fine-tuning implementation</li>
        <li style="list-style-type: none; margin-left: 20px;">- <code>run_all_examples.sh</code> - Script to run all three main examples</li>
        <li><strong>Data Files</strong>:</li>
        <li style="list-style-type: none; margin-left: 20px;">- <code>teacher_student_dialogues.jsonl</code> - Sample dialogue data for DSPy</li>
        <li style="list-style-type: none; margin-left: 20px;">- <code>small_edu_qa.jsonl</code> - Sample Q&A pairs for simple examples</li>
        <li style="list-style-type: none; margin-left: 20px;">- <code>openai_edu_qa_training.jsonl</code> - Sample training data for OpenAI fine-tuning</li>
        <li><strong>Environment Setup</strong>:</li>
        <li style="list-style-type: none; margin-left: 20px;">- <code>environment.yml</code> - Located in the project root directory, contains all necessary dependencies for the conda environment</li>
        <li><strong>All assignment files</strong>:<span>&nbsp;</span><a href="https://github.com/UVU-AI-Innovate/UTTA/tree/main/examples">https://github.com/UVU-AI-Innovate/UTTA/tree/main/examples</a></li>
        <li><strong>External Documentation</strong>:</li>
        <li style="list-style-type: none; margin-left: 20px;">- DSPy documentation:<span>&nbsp;</span><a href="https://dspy-docs.vercel.app/">https://dspy-docs.vercel.app/</a></li>
        <li style="list-style-type: none; margin-left: 20px;">- OpenAI fine-tuning guide:<span>&nbsp;</span><a href="https://platform.openai.com/docs/guides/fine-tuning">https://platform.openai.com/docs/guides/fine-tuning</a></li>
        <li style="list-style-type: none; margin-left: 20px;">- HuggingFace PEFT documentation:<span>&nbsp;</span><a href="https://huggingface.co/docs/peft/index">https://huggingface.co/docs/peft/index</a></li>
    </ul>
    <hr />
    <p><em>Note for instructors: This assignment can be adapted to different technical levels by requiring only certain parts. Part 1 is accessible to most students, while Part 3 requires more advanced ML experience and hardware resources.</em></p>
</div>