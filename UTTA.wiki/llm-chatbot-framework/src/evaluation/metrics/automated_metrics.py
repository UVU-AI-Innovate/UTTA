from typing import List, Dict, Any
import numpy as np
from rouge_score import rouge_scorer
from bert_score import BERTScorer
from sacrebleu.metrics import BLEU
from nltk.translate.meteor_score import meteor_score
import nltk

try:
    nltk.data.find('wordnet')
except LookupError:
    nltk.download('wordnet')

class AutomatedMetrics:
    """Class for automated evaluation metrics of chatbot responses."""
    
    def __init__(self):
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        self.bert_scorer = BERTScorer(lang="en", rescale_with_baseline=True)
        self.bleu = BLEU()
        
    def evaluate_response(self, 
                         generated_response: str, 
                         reference_response: str) -> Dict[str, float]:
        """
        Evaluate a single response using multiple metrics.
        
        Args:
            generated_response: The response generated by the chatbot
            reference_response: The ground truth/reference response
            
        Returns:
            Dictionary containing scores for different metrics
        """
        # Calculate ROUGE scores
        rouge_scores = self.rouge_scorer.score(reference_response, generated_response)
        
        # Calculate BERTScore
        P, R, F1 = self.bert_scorer.score([generated_response], [reference_response])
        bert_score = float(F1.mean())
        
        # Calculate BLEU score
        bleu_score = self.bleu.sentence_score(generated_response, [reference_response]).score / 100.0
        
        # Calculate METEOR score
        meteor = meteor_score([reference_response.split()], generated_response.split())
        
        return {
            'rouge1_f1': rouge_scores['rouge1'].fmeasure,
            'rouge2_f1': rouge_scores['rouge2'].fmeasure,
            'rougeL_f1': rouge_scores['rougeL'].fmeasure,
            'bert_score': bert_score,
            'bleu': bleu_score,
            'meteor': meteor
        }
    
    def evaluate_conversation(self, 
                            conversation_pairs: List[Dict[str, str]]) -> Dict[str, float]:
        """
        Evaluate an entire conversation using multiple metrics.
        
        Args:
            conversation_pairs: List of dictionaries containing generated and reference responses
                             [{'generated': 'bot response', 'reference': 'ground truth'}]
                             
        Returns:
            Dictionary containing aggregated scores across the conversation
        """
        all_scores = []
        for pair in conversation_pairs:
            scores = self.evaluate_response(pair['generated'], pair['reference'])
            all_scores.append(scores)
            
        # Calculate mean scores across all responses
        mean_scores = {}
        for metric in all_scores[0].keys():
            values = [score[metric] for score in all_scores]
            mean_scores[metric] = float(np.mean(values))
            mean_scores[f'{metric}_std'] = float(np.std(values))
            
        return mean_scores 