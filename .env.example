# LlamaIndex Configuration Example .env file
# Copy this file to .env and fill in your values

# LLM Provider Settings
LLAMA_INDEX_LLM_PROVIDER=openai  # options: openai, anthropic, local

# OpenAI Settings (if using OpenAI as provider)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-3.5-turbo  # or gpt-4, etc.
OPENAI_TEMPERATURE=0.7

# Anthropic Settings (if using Anthropic as provider)
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-sonnet-20240229
ANTHROPIC_TEMPERATURE=0.7

# Local Model Settings (if using local LLM)
LOCAL_MODEL_PATH=/path/to/your/local/model
LOCAL_TEMPERATURE=0.7
LOCAL_MAX_TOKENS=2048

# Embedding Model Settings
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Document Processing Settings
LLAMA_INDEX_CHUNK_SIZE=500
LLAMA_INDEX_CHUNK_OVERLAP=50

# Caching Settings
LLAMA_INDEX_ENABLE_CACHE=true
LLAMA_INDEX_CACHE_TTL=3600  # 1 hour in seconds

# Retrieval Settings
LLAMA_INDEX_TOP_K=5
LLAMA_INDEX_SIMILARITY_CUTOFF=0.7

# Directory Settings
# Note: These can be overridden in the code, but can be set here for convenience
# KNOWLEDGE_DIR=/custom/path/to/knowledge_base
# BOOKS_DIR=/custom/path/to/knowledge_base/books
# INDEX_DIR=/custom/path/to/knowledge_base/llama_index
# CACHE_DIR=/custom/path/to/knowledge_base/query_cache 