# Utah Teacher Training Assistant (UTTA)

A comprehensive framework for developing and training chatbots specialized in teacher training and pedagogical assistance.

## ğŸ¯ Project Overview

This framework provides a modular system for building chatbots that can assist in teacher training, offering:
- Pedagogical response generation
- Student reaction simulation
- Teaching strategy analysis
- Scaffolded learning support
- Metacognitive prompting

## ğŸ—ï¸ Project Structure

```
llm-chatbot-framework/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm/              # LLM integration and handling
â”‚   â”‚   â”œâ”€â”€ dspy/        # DSPy-based LLM components
â”‚   â”‚   â””â”€â”€ handlers/    # Various LLM handlers
â”‚   â”œâ”€â”€ knowledge_base/   # Knowledge base integration
â”‚   â”œâ”€â”€ fine_tuning/      # Model fine-tuning components
â”‚   â””â”€â”€ evaluation/       # Evaluation metrics and tools
â”œâ”€â”€ tests/                # Test suites
â”œâ”€â”€ data/                 # Training and evaluation data
â””â”€â”€ requirements.txt      # Project dependencies
```

## ğŸš€ Getting Started

1. Set up the environment:
```bash
conda create -n utta python=3.10
conda activate utta
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

2. Set up model access:

### OpenAI API (Default)
1. Create an OpenAI account at https://platform.openai.com/signup
2. Generate an API key at https://platform.openai.com/api-keys
3. Set your API key:
```bash
export OPENAI_API_KEY='your-api-key'
```

### Alternative Models (Cost-Effective Options)

#### Option 1: Local Ollama Models
1. Install Ollama from https://ollama.ai/
2. Pull and run a model:
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull Mistral (recommended for teaching tasks)
ollama pull mistral

# For better performance but more resources:
ollama pull llama2
```

3. Update the model in your code:
```python
llm = EnhancedDSPyLLMInterface(model_name="ollama/mistral")
```

#### Option 2: Hugging Face Models
1. Install transformers:
```bash
pip install transformers
```

2. Use open-source models:
```python
from transformers import pipeline

# Phi-2 (Microsoft's compact but powerful model)
model = pipeline('text-generation', model='microsoft/phi-2')

# Or SOLAR (Upstage's efficient model)
model = pipeline('text-generation', model='upstage/SOLAR-10.7B-Instruct-v1.0')
```

#### Cost Comparison (as of 2024):
- GPT-3.5-turbo: $0.0010 / 1K tokens
- GPT-4: $0.03 / 1K tokens
- Local models: Free after initial setup
- Phi-2: Free, 2.7B parameters
- Mistral: Free, 7B parameters
- SOLAR: Free, 10.7B parameters

Choose based on your needs:
- Development/Testing: Use local models (Ollama)
- Production/High accuracy: Use OpenAI API
- Balance: Use Phi-2 or Mistral locally

## ğŸ“‹ Development Milestones

### 1ï¸âƒ£ Chatbot Development
- [x] DSPy LLM integration
- [x] Pedagogical language processing
- [x] Student reaction generation
- [x] Teaching analysis

### 2ï¸âƒ£ Knowledge Base Integration
- [ ] Document indexing
- [ ] Context retrieval
- [ ] Knowledge graph integration
- [ ] Semantic search

### 3ï¸âƒ£ Fine-Tuning
- [x] Training data preparation
- [x] Model fine-tuning pipeline
- [x] Evaluation metrics
- [ ] Hyperparameter optimization

### 4ï¸âƒ£ Evaluation
- [x] Automated metrics
- [x] Teaching quality assessment
- [ ] Student engagement analysis
- [ ] Learning outcome evaluation

## ğŸ’» Usage Examples

```python
from src.llm.dspy.handler import EnhancedDSPyLLMInterface

# Initialize the interface (choose your model)
llm = EnhancedDSPyLLMInterface(model_name="gpt-3.5-turbo")  # OpenAI (default)
# llm = EnhancedDSPyLLMInterface(model_name="ollama/mistral")  # Local Mistral
# llm = EnhancedDSPyLLMInterface(model_name="microsoft/phi-2")  # Phi-2

# Generate a teaching response
response = llm.generate_comprehensive_response(
    student_query="What is photosynthesis?",
    student_profile={
        "level": "intermediate",
        "learning_style": "visual"
    }
)

# Access different components of the response
print(response['response'])  # Main teaching response
print(response['scaffolding'])  # Scaffolded learning components
print(response['metacognition'])  # Metacognitive prompts
```

## ğŸ§ª Running Tests

```bash
pytest tests/
```

## ğŸ“š Documentation

Each component includes detailed documentation in its respective directory:
- `src/llm/README.md` - LLM integration details
- `src/knowledge_base/README.md` - Knowledge base setup
- `src/fine_tuning/README.md` - Fine-tuning process
- `src/evaluation/README.md` - Evaluation metrics

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.