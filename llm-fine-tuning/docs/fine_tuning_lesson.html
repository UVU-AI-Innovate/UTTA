<div id="dp-wrapper" class="dp-wrapper">
    <div id="kl_custom_css"></div>
    <div id="dpCustomCss" data-external-style="/css/style.css"></div>

    <h2 class="rad">&nbsp;<i class="fas fa-book-reader" aria-hidden="true"><span class="dp-icon-content" style="display: none;">&nbsp;</span></i> &nbsp;<strong>Read About LLM Fine-Tuning Methods</strong></h2>

    <div>
        <div class="dp-callout dp-callout-placeholder card dp-callout-position-default dp-callout-type-default dp-callout-color-primary w-75 dp-callout-pos-c rounded-1">
            <div class="card-body">
                <p class="card-text">This reading covers three different approaches to fine-tuning Large Language Models (LLMs): DSPy Optimization, OpenAI Fine-Tuning, and HuggingFace Fine-Tuning with LoRA. Each approach offers different trade-offs in terms of complexity,
                    resource requirements, and capabilities. The readings include practical code examples and explanations to help you understand and compare these methods.</p>
            </div>
        </div>
    </div>

    <h3 class="rad"><strong>Introduction</strong></h3>
    <p>Large Language Models (LLMs) have revolutionized natural language processing, but their general-purpose training might not always align perfectly with specific domain needs. Fine-tuning is the process of adapting pre-trained models to perform better
        on specialized tasks with domain-specific data.</p>

    <p>This reading explores three different approaches to fine-tuning LLMs, ranging from lightweight prompt optimization to full model weight updates. Each method offers different trade-offs in terms of resource requirements, control, data needs, and deployment
        options.
    </p>

    <table class="comparison-table">
        <tr>
            <th>Feature</th>
            <th>DSPy</th>
            <th>OpenAI</th>
            <th>HuggingFace</th>
        </tr>
        <tr>
            <td><strong>What changes</strong></td>
            <td>Prompts</td>
            <td>Model weights (cloud)</td>
            <td>Model weights (local)</td>
        </tr>
        <tr>
            <td><strong>Training data needed</strong></td>
            <td>Small (10s)</td>
            <td>Medium (100s-1000s)</td>
            <td>Medium/Large (1000s+)</td>
        </tr>
        <tr>
            <td><strong>Cost</strong></td>
            <td>API calls only</td>
            <td>API + training</td>
            <td>One-time compute</td>
        </tr>
        <tr>
            <td><strong>Setup difficulty</strong></td>
            <td>Simple</td>
            <td>Simple</td>
            <td>Complex</td>
        </tr>
        <tr>
            <td><strong>Control</strong></td>
            <td>Limited</td>
            <td>Medium</td>
            <td>Full</td>
        </tr>
        <tr>
            <td><strong>Hardware required</strong></td>
            <td>None</td>
            <td>None</td>
            <td>GPU recommended</td>
        </tr>
        <tr>
            <td><strong>Deployment</strong></td>
            <td>API calls</td>
            <td>API calls</td>
            <td>Self-host</td>
        </tr>
        <tr>
            <td><strong>Data privacy</strong></td>
            <td>Data shared with API</td>
            <td>Data shared with API</td>
            <td>Data stays local</td>
        </tr>
    </table>

    <h4 class="line"><strong>Reading Assignments</strong></h4>

    <div class="method-section dspy-section">
        <h4><i class="fas fa-lightbulb" aria-hidden="true"></i> DSPy Optimization</h4>

        <p><strong>What is DSPy Optimization?</strong></p>
        <p>
            DSPy optimization is a technique that improves a model's performance by optimizing the <span class="highlight">prompts</span> rather than the model weights. It uses a small dataset to find better instructions that elicit improved responses
            from the underlying model.
        </p>

        <p><strong>Key Characteristics:</strong></p>
        <ul>
            <li>Requires minimal data (as few as 10-20 examples)</li>
            <li>No need for expensive training infrastructure</li>
            <li>Quick to implement and test</li>
            <li>No changes to model weights</li>
            <li>Limited to capabilities already present in base model</li>
            <li>Still requires API calls to the model</li>
        </ul>

        <p><strong>Example Code:</strong></p>
        <pre>
import dspy
from src.dspy.models import EducationalQAModel
from src.dspy.optimizer import DSPyOptimizer
from src.utils.dataset import load_jsonl, split_dataset

# Load educational QA dataset
dataset = load_jsonl("examples/data/educational_qa_sample.jsonl")

# Split into train and test
train_data, test_data = split_dataset(dataset, test_size=0.3)

# Initialize the optimizer with GPT-3.5
optimizer = DSPyOptimizer(
    model=EducationalQAModel(),
    model_name="gpt-3.5-turbo",
    temperature=0.0
)

# Run optimization process
optimizer.optimize(
    train_data=train_data,
    num_iterations=3,
    metric="answer_quality"
)

# Evaluate the optimized model
results = optimizer.evaluate(test_data)
print(f"Accuracy: {results['accuracy']:.2f}")
print(f"Average answer quality: {results['avg_quality']:.2f}")

# Generate answer for a new question
answer = optimizer.predict("What is photosynthesis and why is it important?")
print(f"Answer: {answer}")
</pre>
    </div>

    <div class="method-section openai-section">
        <h4><i class="fas fa-cloud" aria-hidden="true"></i> OpenAI Fine-Tuning</h4>

        <p><strong>What is OpenAI Fine-Tuning?</strong></p>
        <p>
            OpenAI fine-tuning adapts models like GPT-3.5 or GPT-4 by updating their weights through additional training on your specific dataset. This process is managed entirely through OpenAI's cloud infrastructure, making it accessible without requiring specialized
            hardware.
        </p>

        <p><strong>Key Characteristics:</strong></p>
        <ul>
            <li>No local computing resources needed</li>
            <li>Relatively simple API-based workflow</li>
            <li>Actual model weight updates (more powerful than prompt optimization)</li>
            <li>Built-in monitoring and evaluation tools</li>
            <li>Requires more data (hundreds to thousands of examples)</li>
            <li>Higher cost (API calls + training compute)</li>
            <li>Limited control over training process</li>
        </ul>

        <p><strong>Example Code:</strong></p>
        <pre>
import os
import json
from src.openai.finetuner import OpenAIFineTuner
from src.utils.dataset import load_jsonl, split_dataset

# Make sure you have your OpenAI API key set
os.environ["OPENAI_API_KEY"] = "your-api-key-here"

# Load and split dataset
dataset = load_jsonl("examples/data/educational_qa_sample.jsonl")
train_data, test_data = split_dataset(dataset, test_size=0.2)

# Initialize the fine-tuner
finetuner = OpenAIFineTuner(
    base_model="gpt-3.5-turbo",
    suffix="educational-qa",
    n_epochs=3
)

# Define a system prompt that guides the model's behavior
system_prompt = "You are an educational assistant that provides accurate, thorough, and grade-appropriate answers to student questions."

# Prepare the training data in OpenAI's format
formatted_data = finetuner.prepare_data(
    dataset=train_data,
    system_prompt=system_prompt,
    input_key="question",
    output_key="answer"
)

# Save formatted data to a file
data_path = "training_data.jsonl"
with open(data_path, "w") as f:
    for item in formatted_data:
        f.write(json.dumps(item) + "\n")

# Start the fine-tuning job
job_id = finetuner.start_finetuning(
    data_path=data_path,
    batch_size=8,
    learning_rate_multiplier=0.1
)

print(f"Fine-tuning job started with ID: {job_id}")

# Wait for the job to complete (this could take hours)
result = finetuner.wait_for_completion(job_id)
fine_tuned_model = result.get("fine_tuned_model")

# Generate an answer using the fine-tuned model
question = "What causes the seasons on Earth?"
answer = finetuner.generate(
    model_name=fine_tuned_model,
    question=question,
    system_prompt=system_prompt
)

print(f"Question: {question}")
print(f"Answer: {answer}")
</pre>
    </div>

    <div class="method-section huggingface-section">
        <h4><i class="fas fa-cogs" aria-hidden="true"></i> HuggingFace Fine-Tuning with LoRA</h4>

        <p><strong>What is HuggingFace Fine-Tuning with LoRA?</strong></p>
        <p>
            HuggingFace fine-tuning involves locally training open-source models using Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation (LoRA). LoRA keeps most of the model frozen while adding small trainable "adapter" matrices, making fine-tuning
            possible even on consumer-grade hardware.
        </p>

        <p><strong>Key Characteristics:</strong></p>
        <ul>
            <li>Complete control over the training process</li>
            <li>Data never leaves your infrastructure (privacy)</li>
            <li>One-time compute cost (no ongoing API fees)</li>
            <li>Can deploy models anywhere after training</li>
            <li>Access to the full range of open models</li>
            <li>Requires GPU hardware</li>
            <li>More complex setup and maintenance</li>
            <li>Technical expertise needed</li>
        </ul>

        <p><strong>Example Code:</strong></p>
        <pre>
import torch
from datasets import Dataset
from src.huggingface.finetuner import HuggingFaceFineTuner
from src.utils.dataset import load_jsonl, split_dataset

# Check if GPU is available
if not torch.cuda.is_available():
    print("Warning: No GPU detected. Fine-tuning will be very slow!")

# Load and split dataset
dataset = load_jsonl("examples/data/educational_qa_sample.jsonl")
train_data, val_data = split_dataset(dataset, test_size=0.2)

# Initialize the fine-tuner with a smaller model for this example
finetuner = HuggingFaceFineTuner(
    base_model="meta-llama/Llama-2-7b-hf",  # You could use a smaller model if needed
    output_dir="lora-educational-qa",
    lora_r=8,              # Rank of the update matrices
    lora_alpha=16,         # Scaling factor
    lora_dropout=0.05,     # Dropout probability for LoRA layers
    use_4bit=True          # Use 4-bit quantization to reduce memory usage
)

# Define a system prompt for the model
system_prompt = "You are a helpful educational assistant. Provide clear, accurate, and comprehensive answers to student questions."

# Prepare the training data in the format needed for the model
train_dataset = finetuner.prepare_data(
    dataset=train_data,
    system_prompt=system_prompt,
    input_key="question",
    output_key="answer"
)

# Prepare validation data
val_dataset = finetuner.prepare_data(
    dataset=val_data,
    system_prompt=system_prompt,
    input_key="question",
    output_key="answer"
)

# Start the fine-tuning process
training_results = finetuner.fine_tune(
    train_dataset=train_dataset,
    val_dataset=val_dataset,
    num_train_epochs=3,
    learning_rate=2e-4,
    batch_size=4,
    gradient_accumulation_steps=4
)

print(f"Training loss: {training_results['train_loss']}")
print(f"Validation loss: {training_results['eval_loss']}")

# Generate an answer using the fine-tuned model
question = "How do black holes form?"
answer = finetuner.generate(
    question=question,
    system_prompt=system_prompt,
    max_new_tokens=256,
    temperature=0.7
)

print(f"Question: {question}")
print(f"Answer: {answer}")
</pre>
    </div>

    <h4 class="line"><strong>Guided Reading Questions</strong></h4>

    <ol>
        <li><strong>Comparing Approaches:</strong> What are the key differences between DSPy Optimization, OpenAI Fine-Tuning, and HuggingFace Fine-Tuning with LoRA in terms of what actually changes in the model?</li>
        <li><strong>Data Requirements:</strong> How do the data requirements differ across the three fine-tuning approaches? Why does each approach require different amounts of data?</li>
        <li><strong>Resource Considerations:</strong> What hardware and infrastructure are needed for each approach? What are the cost implications of each method?</li>
        <li><strong>Use Cases:</strong> For each fine-tuning method, identify a specific use case where it would be the most appropriate choice. Explain your reasoning.</li>
        <li><strong>Implementation Complexity:</strong> Compare the implementation complexity of the three approaches based on the code examples provided. Which aspects of each implementation might be challenging for new users?</li>
        <li><strong>Privacy and Control:</strong> How do the three approaches differ in terms of data privacy and control over the fine-tuning process?</li>
        <li><strong>Scaling Considerations:</strong> How would each approach scale if you needed to fine-tune multiple models or update your models regularly?</li>
        <li><strong>Code Analysis:</strong> Looking at the example code for each method, identify the key components and explain their purpose in the fine-tuning process.</li>
        <li><strong>Optimization Metrics:</strong> What metrics are used to evaluate the performance of the fine-tuned models in each approach? How might these metrics guide further improvements?</li>
        <li><strong>Deployment Strategy:</strong> After fine-tuning with each method, what would the deployment process look like? Consider aspects like hosting requirements, API integration, and maintenance.</li>
    </ol>

    <h4 class="line"><strong>Official References</strong></h4>

    <div class="references">
        <div class="method-refs">
            <h4><i class="fas fa-lightbulb" aria-hidden="true"></i> DSPy References</h4>
            <ul>
                <li>Khattab, O., Santhanam, K., Li, X., Hall, D., Liang, P., Potts, C., & Zaharia, M. (2023). <a href="https://arxiv.org/abs/2212.14024" target="_blank">Demonstrates: Compiling Prompts through Selective Automatic Demonstrations.</a> arXiv
                    preprint arXiv:2212.14024.</li>
                <li>Stanford NLP Group. (2023). <a href="https://github.com/stanfordnlp/dspy" target="_blank">DSPy: Programming with Foundation Models</a>. GitHub Repository.</li>
                <li>Khattab, O., Santhanam, K., Potts, C., & Zaharia, M. (2023). <a href="https://arxiv.org/abs/2310.03714" target="_blank">DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</a>. arXiv preprint arXiv:2310.03714.</li>
                <li>Stanford NLP Group. (2023). <a href="https://dspy-docs.vercel.app/" target="_blank">DSPy Documentation</a>. Official Documentation.</li>
            </ul>
        </div>

        <div class="method-refs">
            <h4><i class="fas fa-cloud" aria-hidden="true"></i> OpenAI Fine-Tuning References</h4>
            <ul>
                <li>OpenAI. (2023). <a href="https://platform.openai.com/docs/guides/fine-tuning" target="_blank">Fine-tuning Guide</a>. OpenAI Documentation.</li>
                <li>OpenAI. (2023). <a href="https://platform.openai.com/docs/api-reference/fine-tuning" target="_blank">Fine-tuning API Reference</a>. OpenAI API Documentation.</li>
                <li>Brown, T.B., Mann, B., Ryder, N., Subbiah, M., et al. (2020). <a href="https://arxiv.org/abs/2005.14165" target="_blank">Language Models are Few-Shot Learners</a>. arXiv preprint arXiv:2005.14165.</li>
                <li>OpenAI. (2023). <a href="https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates" target="_blank">GPT-3.5 Turbo Fine-tuning and API Updates</a>. OpenAI Blog.</li>
                <li>OpenAI. (2022). <a href="https://openai.com/research/instruction-following" target="_blank">Aligning Language Models to Follow Instructions</a>. OpenAI Research.</li>
            </ul>
        </div>

        <div class="method-refs">
            <h4><i class="fas fa-cogs" aria-hidden="true"></i> HuggingFace Fine-Tuning with LoRA References</h4>
            <ul>
                <li>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2022). <a href="https://arxiv.org/abs/2106.09685" target="_blank">LoRA: Low-Rank Adaptation of Large Language Models</a>. arXiv preprint arXiv:2106.09685.</li>
                <li>HuggingFace. (2023). <a href="https://huggingface.co/docs/peft/index" target="_blank">Parameter-Efficient Fine-Tuning (PEFT)</a>. HuggingFace Documentation.</li>
                <li>HuggingFace. (2023). <a href="https://huggingface.co/docs/transformers/index" target="_blank">Transformers Documentation</a>. HuggingFace.</li>
                <li>Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). <a href="https://arxiv.org/abs/2302.13971" target="_blank">QLoRA: Efficient Finetuning of Quantized LLMs</a>. arXiv preprint arXiv:2302.13971.</li>
                <li>HuggingFace. (2023). <a href="https://huggingface.co/blog/lora" target="_blank">LoRA: Low-Rank Adaptation Technique for Efficient Fine-Tuning of Large Models</a>. HuggingFace Blog.</li>
                <li>HuggingFace. (2023). <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/trainer" target="_blank">Trainer</a>. HuggingFace Transformers Documentation.</li>
            </ul>
        </div>

        <div class="method-refs">
            <h4><i class="fas fa-book" aria-hidden="true"></i> General References on Fine-Tuning</h4>
            <ul>
                <li>Bommasani, R., Hudson, D. A., Adeli, E., et al. (2021). <a href="https://arxiv.org/abs/2108.07258" target="_blank">On the Opportunities and Risks of Foundation Models</a>. arXiv preprint arXiv:2108.07258.</li>
                <li>Ouyang, L., Wu, J., Jiang, X., Almeida, D., et al. (2022). <a href="https://arxiv.org/abs/2203.02155" target="_blank">Training Language Models to Follow Instructions with Human Feedback</a>. arXiv preprint arXiv:2203.02155.</li>
                <li>Wolf, T., Debut, L., Sanh, V., Chaumond, J., et al. (2020). <a href="https://aclanthology.org/2020.emnlp-demos.6/" target="_blank">Transformers: State-of-the-Art Natural Language Processing</a>. Proceedings of the 2020 Conference on Empirical
                    Methods in Natural Language Processing: System Demonstrations.</li>
                <li>Raffel, C., Shazeer, N., Roberts, A., Lee, K., et al. (2020). <a href="https://arxiv.org/abs/1910.10683" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>. Journal of Machine Learning
                    Research.
                </li>
            </ul>
        </div>
    </div>

    <hr class="rule" style="background-color: #000000; height: 4px;" />
</div>